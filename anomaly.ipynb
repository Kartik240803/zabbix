{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481dad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zabbix_db as zb\n",
    "import pandas as pd\n",
    "from adtk.detector import LevelShiftAD\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_config = {\n",
    "    'db_type': 'mysql',  # or 'postgresql'\n",
    "    'host': '140.238.230.93',\n",
    "    'port': 3306,  # 5432 for PostgreSQL\n",
    "    'database': 'zabbix',\n",
    "    'user': 'kartik',\n",
    "    'password': 'Kartik@24082003'\n",
    "}\n",
    "\n",
    "# Sample hostname to query\n",
    "hostname = 'Zabbix server'\n",
    "metric_name = 'CPU utilization'  # Example metric name\n",
    "# metric_name = 'Host name of Zabbix agent running'\n",
    "\n",
    "\n",
    "with zb.ZabbixDB(**db_config) as zbx:\n",
    "    result = zbx.get_metric_data(\n",
    "        hostname=hostname,\n",
    "        metric_name=metric_name,\n",
    "        time_from=1746793913,  # Example start time (Unix timestamp)\n",
    "        time_to=1749549165,  # Example end time (Unix timestamp)\n",
    "          # Example statistical measure\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "\n",
    "    # result = zbx.get_host_by_metric(metric_name=metric_name,time_from=1749032410, time_to=1749118810, statistical_measure='max')\n",
    "    # # result = zbx.get_item_detail(item_name='CPU nice time', hostname=hostname)\n",
    "    # print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463bf7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(result['data'], columns=['clock', 'value'])\n",
    "print(data)\n",
    "data1 = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8582938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert 'clock' from Unix timestamp to datetime and set as index\n",
    "# data['timestamp'] = pd.to_datetime(data['clock'], unit='s')\n",
    "# data_series = data.set_index('timestamp')['value']\n",
    "\n",
    "# # Initialize the LevelShiftAD detector\n",
    "# model = LevelShiftAD(c=1.5, side='both', window=5)\n",
    "\n",
    "# # Detect anomalies\n",
    "# anomalies = model.fit_detect(data_series)\n",
    "\n",
    "# # Extract timestamps and values where anomalies are found\n",
    "# anomaly_timestamps = anomalies[anomalies == 1].index\n",
    "# anomaly_values = data_series.loc[anomaly_timestamps]\n",
    "\n",
    "# # Create a DataFrame with anomalies\n",
    "# anomaly_df = pd.DataFrame({'timestamp': anomaly_timestamps, 'value': anomaly_values})\n",
    "\n",
    "# # Save anomalies to CSV\n",
    "# anomaly_df.to_csv('anomalies.csv', index=False)\n",
    "# print(\"Anomalies saved to anomalies.csv\")\n",
    "\n",
    "# # Visualize the data with anomalies\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(data_series.index, data_series, label='Data', color='#36A2EB')\n",
    "# plt.scatter(anomaly_timestamps, anomaly_values, color='#FF6384', label='Anomalies')\n",
    "# plt.xlabel('Timestamp')\n",
    "# plt.ylabel('Value (e.g., CPU Utilization)')\n",
    "# plt.title('Anomaly Detection in IT Monitoring Data')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1ab162",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70760f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure df is sorted by index (which is already datetime 'clock')\n",
    "df = df.sort_index()\n",
    "\n",
    "# Perform STL decomposition\n",
    "stl = STL(\n",
    "    df['value'],\n",
    "    period=60,      # Hourly seasonality (60 minutes)\n",
    "    seasonal=7,     # Seasonal smoother length\n",
    "    trend=121,      # Trend smoother length\n",
    "    robust=True\n",
    ")\n",
    "decomposition = stl.fit()\n",
    "\n",
    "# Extract residual and calculate standard deviation\n",
    "residual = decomposition.resid\n",
    "std_resid = residual.std()\n",
    "\n",
    "# Set anomaly threshold (3 deviations)\n",
    "anomaly_threshold = 5 * std_resid\n",
    "\n",
    "# Find anomalies where |residual| > threshold\n",
    "anomaly_indices = residual[abs(residual) > anomaly_threshold].index\n",
    "\n",
    "# Create output DataFrame with 'clock' (Unix timestamps) and 'value'\n",
    "anomalies = pd.DataFrame({\n",
    "    'clock': (anomaly_indices.astype(int) // 10**9).astype(int),\n",
    "    'value': df['value'].loc[anomaly_indices]\n",
    "}, index=anomaly_indices)\n",
    "\n",
    "# Reset index to remove 'clock' as an index, keeping it only as a column\n",
    "anomalies = anomalies.reset_index(drop=True)\n",
    "\n",
    "# Sort by 'clock' for clarity\n",
    "anomalies = anomalies.sort_values('clock')\n",
    "\n",
    "# Print the anomalies DataFrame\n",
    "print(anomalies)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df.index, df['value'], label='Data', color='#36A2EB')\n",
    "plt.scatter(anomalies['clock'].apply(lambda x: pd.Timestamp(x, unit='s')), anomalies['value'], color='#FF6384', label='Anomalies')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Value (e.g., CPU Utilization)')\n",
    "plt.title('Anomaly Detection with STL (trendstl)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39feeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab4a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.robust import mad\n",
    "\n",
    "# --- Step 1: Preprocess Full Data ---\n",
    "# Assume df has datetime index and 'value' column\n",
    "\n",
    "df = df.sort_index()\n",
    "df = df.asfreq('1T')  # 1-minute data; adjust if needed\n",
    "df['value'] = df['value'].interpolate(method='time')\n",
    "\n",
    "# --- Step 2: STL Decomposition on Full Dataset ---\n",
    "period = 1440        # 1 day (for 1-min data)\n",
    "seasonal = 61\n",
    "trend = 1921         # Must be > period and odd\n",
    "\n",
    "stl = STL(df['value'], period=period, seasonal=seasonal, trend=trend, robust=True)\n",
    "result = stl.fit()\n",
    "\n",
    "resid = result.resid\n",
    "threshold = 5 * mad(resid)\n",
    "\n",
    "# --- Step 3: Get Last 1-Hour Residuals ---\n",
    "last_hour_start = df.index.max() - pd.Timedelta(hours=1)\n",
    "last_hour_resid = resid[last_hour_start:]\n",
    "\n",
    "# --- Step 4: Pattern Check (Smart Suppression of Common Behavior) ---\n",
    "def is_repeating_pattern(ts_index, df, tolerance=0.1):\n",
    "    \"\"\"\n",
    "    Checks whether a similar value occurred often historically around the same time of day.\n",
    "    Suppresses anomaly if it is part of a repeating pattern.\n",
    "    \"\"\"\n",
    "    matches = 0\n",
    "    current_value = df.loc[ts_index, 'value']\n",
    "    time_of_day = ts_index.time()\n",
    "    \n",
    "    # Create a time window of +/- 3 minutes around that time of day for historical days\n",
    "    for day_offset in range(1, 31):  # last 30 days\n",
    "        try:\n",
    "            compare_time = ts_index - pd.Timedelta(days=day_offset)\n",
    "            compare_window = df.between_time(\n",
    "                (pd.Timestamp.combine(compare_time.date(), time_of_day) - pd.Timedelta(minutes=3)).time(),\n",
    "                (pd.Timestamp.combine(compare_time.date(), time_of_day) + pd.Timedelta(minutes=3)).time()\n",
    "            )\n",
    "\n",
    "            if not compare_window.empty:\n",
    "                if np.any(abs(compare_window['value'] - current_value) < tolerance * current_value):\n",
    "                    matches += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return matches >= 2  # suppress if value occurs >=2 times historically\n",
    "\n",
    "# --- Step 5: Identify True Anomalies (Filter Repeating Patterns) ---\n",
    "anomalies = []\n",
    "for ts in last_hour_resid.index:\n",
    "    if abs(last_hour_resid.loc[ts]) > threshold:\n",
    "        if not is_repeating_pattern(ts, df):\n",
    "            anomalies.append((ts, df.loc[ts, 'value']))\n",
    "\n",
    "# --- Step 6: Format Anomalies as DataFrame ---\n",
    "anomaly_df = pd.DataFrame(anomalies, columns=['timestamp', 'value'])\n",
    "anomaly_df['clock'] = anomaly_df['timestamp'].astype(np.int64) // 10**9\n",
    "anomaly_df = anomaly_df[['clock', 'value']]\n",
    "\n",
    "print(\"\\nDetected Anomalies (Last 1 Hour, Pattern-Filtered):\")\n",
    "print(anomaly_df)\n",
    "\n",
    "# --- Step 7: Plot Full Data + Anomalies ---\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df.index, df['value'], label='Full Data', color='#36A2EB')\n",
    "plt.scatter(\n",
    "    anomaly_df['clock'].apply(lambda x: pd.to_datetime(x, unit='s')),\n",
    "    anomaly_df['value'],\n",
    "    color='#FF6384', label='Filtered Anomalies'\n",
    ")\n",
    "plt.axvspan(last_hour_start, df.index.max(), color='gray', alpha=0.1, label='Last Hour')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Full STL Decomposition + Filtered Anomaly Detection (Last 1 Hour)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Step 8: STL Components Plot (Optional) ---\n",
    "result.plot()\n",
    "plt.suptitle('STL Decomposition of Full Dataset', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b04f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.robust import mad\n",
    "\n",
    "# --- Load & preprocess ---\n",
    "df = df.sort_index()\n",
    "df = df.asfreq('1T')  # 1-min frequency\n",
    "df['value'] = df['value'].interpolate(method='time')\n",
    "\n",
    "# --- Extract last 1-hour window first ---\n",
    "now = df.index.max()\n",
    "last_hour_start = now - pd.Timedelta(hours=1)\n",
    "\n",
    "# Split data: historical for pattern learning, recent for anomaly detection\n",
    "df_historic = df.loc[df.index < last_hour_start].copy()\n",
    "df_recent = df.loc[df.index >= last_hour_start].copy()\n",
    "\n",
    "# --- Quick STL on recent data only (much faster) ---\n",
    "if len(df_recent) >= 60:  # Need minimum points for STL\n",
    "    period = min(60, len(df_recent) // 2)  # Adaptive period for short sequences\n",
    "    stl_recent = STL(df_recent['value'], period=period, seasonal=7, trend=None, robust=True)\n",
    "    res_recent = stl_recent.fit()\n",
    "    resid_recent = res_recent.resid\n",
    "else:\n",
    "    # Fallback: simple detrending for very short sequences\n",
    "    resid_recent = df_recent['value'] - df_recent['value'].rolling(window=5, center=True).mean()\n",
    "\n",
    "# --- Vectorized pattern analysis ---\n",
    "df_historic['minute'] = df_historic.index.hour * 60 + df_historic.index.minute\n",
    "df_recent['minute'] = df_recent.index.hour * 60 + df_recent.index.minute\n",
    "\n",
    "# Precompute historical stats (vectorized)\n",
    "stats = df_historic.groupby('minute')['value'].agg(['median', mad]).rename(columns={'mad': 'mad_val'})\n",
    "\n",
    "# --- Vectorized anomaly detection ---\n",
    "# Global threshold from recent residuals\n",
    "global_thresh = 3 * mad(resid_recent.dropna())\n",
    "\n",
    "# Merge recent data with historical stats\n",
    "df_recent_merged = df_recent.merge(stats, left_on='minute', right_index=True, how='left')\n",
    "\n",
    "# Fill missing stats with global median/mad for minutes not in historical data\n",
    "df_recent_merged['median'].fillna(df_historic['value'].median(), inplace=True)\n",
    "df_recent_merged['mad_val'].fillna(mad(df_historic['value']), inplace=True)\n",
    "\n",
    "# Vectorized anomaly conditions\n",
    "resid_condition = np.abs(resid_recent) > global_thresh\n",
    "pattern_condition = np.abs(df_recent_merged['value'] - df_recent_merged['median']) > 2.5 * df_recent_merged['mad_val']\n",
    "\n",
    "# Combine conditions\n",
    "anomaly_mask = resid_condition & pattern_condition\n",
    "\n",
    "# Extract anomalies\n",
    "anomaly_timestamps = df_recent.index[anomaly_mask]\n",
    "anomaly_values = df_recent['value'][anomaly_mask]\n",
    "\n",
    "# --- Create output DataFrame ---\n",
    "if len(anomaly_timestamps) > 0:\n",
    "    anomaly_df = pd.DataFrame({\n",
    "        'timestamp': anomaly_timestamps,\n",
    "        'value': anomaly_values\n",
    "    })\n",
    "    anomaly_df['clock'] = anomaly_df['timestamp'].astype(np.int64) // 10**9\n",
    "    anomaly_df = anomaly_df[['clock', 'value']]\n",
    "else:\n",
    "    anomaly_df = pd.DataFrame(columns=['clock', 'value'])\n",
    "\n",
    "print(f\"\\nDetected Anomalies (Last 1 Hour, Optimized): {len(anomaly_df)} anomalies\")\n",
    "print(anomaly_df)\n",
    "\n",
    "# --- Optimized Plot ---\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot only recent data for speed (or sample historical data)\n",
    "recent_plot_data = df.tail(2880) if len(df) > 2880 else df  # Last 2 days max\n",
    "plt.plot(recent_plot_data.index, recent_plot_data['value'], label='Data', color='#36A2EB', alpha=0.7)\n",
    "\n",
    "if len(anomaly_df) > 0:\n",
    "    plt.scatter(\n",
    "        pd.to_datetime(anomaly_df['clock'], unit='s'),\n",
    "        anomaly_df['value'], \n",
    "        color='#FF6384', \n",
    "        label=f'Anomalies ({len(anomaly_df)})',\n",
    "        s=50, \n",
    "        zorder=5\n",
    "    )\n",
    "\n",
    "plt.axvspan(last_hour_start, now, color='gray', alpha=0.1, label='Detection Window')\n",
    "plt.title('Fast Anomaly Detection with Pattern Suppression')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Performance info ---\n",
    "print(f\"\\nProcessing summary:\")\n",
    "print(f\"Historical data points: {len(df_historic):,}\")\n",
    "print(f\"Recent data points: {len(df_recent):,}\")\n",
    "print(f\"Unique minute patterns: {len(stats):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data1\n",
    "df.head()\n",
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7487e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.robust import mad\n",
    "\n",
    "# --- Load & preprocess ---\n",
    "df = df.sort_index()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clock'] = pd.to_datetime(df['clock'], unit='s')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cfba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('clock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d7437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not isinstance(df.index, pd.DatetimeIndex):\n",
    "    df = df.set_index('clock')\n",
    "df = df.asfreq('1min')  # 1-min frequency\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f608c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.robust import mad\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def ensure_odd_and_valid(value, minimum=3):\n",
    "    \"\"\"Ensure value is odd and >= minimum\"\"\"\n",
    "    value = max(minimum, int(value))\n",
    "    return value if value % 2 == 1 else value + 1\n",
    "\n",
    "def calculate_stl_parameters(data_length, base_period=1440):\n",
    "    \"\"\"Calculate valid STL parameters based on data length\"\"\"\n",
    "    # Ensure we have enough data for meaningful decomposition\n",
    "    if data_length < 60:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Period: Try to use daily pattern (1440 min), but adapt for short data\n",
    "    period = min(base_period, max(2, data_length // 4))\n",
    "    \n",
    "    # Seasonal: Should be odd and >= 3, typically period/4 to period/2\n",
    "    seasonal_base = max(3, period // 4)\n",
    "    seasonal = ensure_odd_and_valid(seasonal_base, 3)\n",
    "    \n",
    "    # Trend: Must be odd, >= 3, and > period\n",
    "    # Rule of thumb: 1.5 * period, but ensure it's not too large\n",
    "    trend_base = max(period + 2, int(1.5 * period))\n",
    "    trend_base = min(trend_base, data_length // 2)  # Don't exceed half the data length\n",
    "    trend = ensure_odd_and_valid(trend_base, period + 2)\n",
    "    \n",
    "    # Final validation\n",
    "    if trend <= period:\n",
    "        trend = ensure_odd_and_valid(period + 2, period + 2)\n",
    "    \n",
    "    # If trend is still too large, reduce period\n",
    "    if trend >= data_length // 2:\n",
    "        period = max(2, data_length // 6)\n",
    "        trend = ensure_odd_and_valid(period + 2, period + 2)\n",
    "        seasonal = ensure_odd_and_valid(max(3, period // 4), 3)\n",
    "    \n",
    "    return period, seasonal, trend\n",
    "\n",
    "def chunked_stl_decomposition(data, chunk_size_hours=24, overlap_hours=2):\n",
    "    \"\"\"\n",
    "    Process STL in overlapping chunks with improved parameter handling\n",
    "    \"\"\"\n",
    "    chunk_size = chunk_size_hours * 60  # Convert to minutes\n",
    "    overlap_size = overlap_hours * 60\n",
    "    residuals = pd.Series(index=data.index, dtype=float)\n",
    "    \n",
    "    start_idx = 0\n",
    "    total_chunks = (len(data) + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    print(f\"Processing {total_chunks} chunks...\")\n",
    "    \n",
    "    for chunk_num in range(total_chunks):\n",
    "        print(f\"Processing chunk {chunk_num + 1}/{total_chunks}...\")\n",
    "        \n",
    "        # Define chunk boundaries\n",
    "        end_idx = min(start_idx + chunk_size + overlap_size, len(data))\n",
    "        chunk_data = data.iloc[start_idx:end_idx]\n",
    "        \n",
    "        if len(chunk_data) < 60:  # Skip if too small for meaningful STL\n",
    "            print(f\"  Chunk {chunk_num + 1} too small ({len(chunk_data)} points), using simple detrending...\")\n",
    "            rolling_mean = chunk_data.rolling(window=min(10, len(chunk_data)//2), center=True).mean()\n",
    "            chunk_residuals = chunk_data - rolling_mean\n",
    "            \n",
    "            if chunk_num == 0:\n",
    "                residuals.iloc[start_idx:end_idx] = chunk_residuals\n",
    "            else:\n",
    "                skip_start = start_idx + overlap_size\n",
    "                residuals.iloc[skip_start:end_idx] = chunk_residuals.iloc[overlap_size:]\n",
    "            \n",
    "            start_idx = end_idx - overlap_size\n",
    "            continue\n",
    "        \n",
    "        # Calculate valid STL parameters\n",
    "        period, seasonal, trend = calculate_stl_parameters(len(chunk_data))\n",
    "        \n",
    "        if period is None:\n",
    "            print(f\"  Chunk {chunk_num + 1} using simple detrending (insufficient data)...\")\n",
    "            rolling_mean = chunk_data.rolling(window=min(20, len(chunk_data)//3), center=True).mean()\n",
    "            chunk_residuals = chunk_data - rolling_mean\n",
    "        else:\n",
    "            print(f\"  STL params - Period: {period}, Seasonal: {seasonal}, Trend: {trend}\")\n",
    "            \n",
    "            try:\n",
    "                stl = STL(chunk_data, period=period, seasonal=seasonal, trend=trend, robust=True)\n",
    "                res = stl.fit()\n",
    "                chunk_residuals = res.resid\n",
    "                print(f\"  STL decomposition successful\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  STL failed for chunk {chunk_num + 1}, using simple detrending: {e}\")\n",
    "                # Fallback: simple detrending\n",
    "                rolling_mean = chunk_data.rolling(window=min(60, len(chunk_data)//4), center=True).mean()\n",
    "                chunk_residuals = chunk_data - rolling_mean\n",
    "        \n",
    "        # Store residuals (avoid overlap except for first chunk)\n",
    "        if chunk_num == 0:\n",
    "            residuals.iloc[start_idx:end_idx] = chunk_residuals\n",
    "        else:\n",
    "            skip_start = start_idx + overlap_size\n",
    "            residuals.iloc[skip_start:end_idx] = chunk_residuals.iloc[overlap_size:]\n",
    "        \n",
    "        # Move to next chunk\n",
    "        start_idx = end_idx - overlap_size\n",
    "    \n",
    "    return residuals\n",
    "\n",
    "def compute_adaptive_stats(data, window_days=30):\n",
    "    \"\"\"Compute adaptive statistics using rolling windows\"\"\"\n",
    "    window_size = window_days * 1440  # Convert days to minutes\n",
    "    \n",
    "    stats_list = []\n",
    "    for minute in range(1440):  # All minutes in a day\n",
    "        minute_data = data[data['minute'] == minute]['value']\n",
    "        \n",
    "        if len(minute_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Use rolling statistics if enough data\n",
    "        if len(minute_data) > window_size:\n",
    "            rolling_median = minute_data.rolling(window=window_size, min_periods=window_size//2).median()\n",
    "            rolling_mad = minute_data.rolling(window=window_size, min_periods=window_size//2).apply(lambda x: mad(x) if len(x) > 0 else 0)\n",
    "            \n",
    "            # Take the last (most recent) values\n",
    "            median_val = rolling_median.iloc[-1] if not rolling_median.empty else minute_data.median()\n",
    "            mad_val = rolling_mad.iloc[-1] if not rolling_mad.empty else mad(minute_data)\n",
    "        else:\n",
    "            # Use all historical data if not enough for rolling window\n",
    "            median_val = minute_data.median()\n",
    "            mad_val = mad(minute_data) if len(minute_data) > 1 else minute_data.std()\n",
    "            \n",
    "        stats_list.append({'minute': minute, 'median': median_val, 'mad_val': mad_val})\n",
    "    \n",
    "    return pd.DataFrame(stats_list).set_index('minute')\n",
    "\n",
    "def detect_anomalies(df):\n",
    "    \"\"\"\n",
    "    Main anomaly detection function with improved robustness\n",
    "    \"\"\"\n",
    "    print(f\"Processing {len(df):,} data points...\")\n",
    "    \n",
    "    # Ensure data is sorted by time\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    # Interpolate missing values\n",
    "    df['value'] = df['value'].interpolate(method='time')\n",
    "    \n",
    "    # --- STL decomposition ---\n",
    "    if len(df) > 10080:  # More than 1 week of data\n",
    "        print(\"Large dataset detected, using chunked processing...\")\n",
    "        residuals = chunked_stl_decomposition(df['value'])\n",
    "    else:\n",
    "        print(\"Small dataset, using full STL...\")\n",
    "        period, seasonal, trend = calculate_stl_parameters(len(df))\n",
    "        \n",
    "        if period is not None:\n",
    "            print(f\"STL params - Period: {period}, Seasonal: {seasonal}, Trend: {trend}\")\n",
    "            try:\n",
    "                stl = STL(df['value'], period=period, seasonal=seasonal, trend=trend, robust=True)\n",
    "                res = stl.fit()\n",
    "                residuals = res.resid\n",
    "                print(\"STL decomposition successful\")\n",
    "            except Exception as e:\n",
    "                print(f\"STL failed, using simple detrending: {e}\")\n",
    "                rolling_mean = df['value'].rolling(window=min(1440, len(df)//4), center=True).mean()\n",
    "                residuals = df['value'] - rolling_mean\n",
    "        else:\n",
    "            print(\"Dataset too small for STL, using simple detrending...\")\n",
    "            rolling_mean = df['value'].rolling(window=min(60, len(df)//4), center=True).mean()\n",
    "            residuals = df['value'] - rolling_mean\n",
    "    \n",
    "    # --- Pattern analysis ---\n",
    "    print(\"Computing historical patterns...\")\n",
    "    df['minute'] = df.index.hour * 60 + df.index.minute\n",
    "    \n",
    "    # Compute adaptive statistics\n",
    "    stats = compute_adaptive_stats(df)\n",
    "    \n",
    "    # --- Anomaly detection ---\n",
    "    print(\"Detecting anomalies...\")\n",
    "    \n",
    "    # Global threshold\n",
    "    global_thresh = 3 * mad(residuals.dropna()) if len(residuals.dropna()) > 0 else 3 * df['value'].std()\n",
    "    print(f\"Global residual threshold: {global_thresh:.3f}\")\n",
    "    \n",
    "    # Merge with historical stats\n",
    "    df_merged = df.merge(stats, left_on='minute', right_index=True, how='left')\n",
    "    \n",
    "    # Fill missing stats with global values\n",
    "    global_median = df['value'].median()\n",
    "    global_mad = mad(df['value']) if len(df) > 1 else df['value'].std()\n",
    "    df_merged['median'].fillna(global_median, inplace=True)\n",
    "    df_merged['mad_val'].fillna(global_mad, inplace=True)\n",
    "    \n",
    "    # Vectorized anomaly conditions\n",
    "    resid_condition = np.abs(residuals) > global_thresh\n",
    "    pattern_condition = np.abs(df_merged['value'] - df_merged['median']) > 2.5 * df_merged['mad_val']\n",
    "    \n",
    "    # Additional condition: significant deviation from recent trend\n",
    "    recent_trend = df['value'].rolling(window=min(60, len(df)//10), center=True).mean()\n",
    "    trend_std = df['value'].rolling(window=min(1440, len(df)//2)).std()\n",
    "    trend_condition = np.abs(df['value'] - recent_trend) > 3 * trend_std\n",
    "    \n",
    "    # Combine all conditions (any two must be true for robustness)\n",
    "    condition_sum = (resid_condition.astype(int) + \n",
    "                    pattern_condition.astype(int) + \n",
    "                    trend_condition.astype(int))\n",
    "    anomaly_mask = condition_sum >= 2\n",
    "    \n",
    "    # Extract anomalies\n",
    "    anomaly_timestamps = df.index[anomaly_mask]\n",
    "    anomaly_values = df['value'][anomaly_mask]\n",
    "    \n",
    "    # Create output DataFrame\n",
    "    if len(anomaly_timestamps) > 0:\n",
    "        anomaly_df = pd.DataFrame({\n",
    "            'timestamp': anomaly_timestamps,\n",
    "            'value': anomaly_values\n",
    "        })\n",
    "        anomaly_df['clock'] = anomaly_df['timestamp'].astype(np.int64) // 10**9\n",
    "        anomaly_df = anomaly_df[['clock', 'value']]\n",
    "    else:\n",
    "        anomaly_df = pd.DataFrame(columns=['clock', 'value'])\n",
    "    \n",
    "    print(f\"\\nDetected Anomalies: {len(anomaly_df):,}\")\n",
    "    print(f\"Anomaly rate: {len(anomaly_df)/len(df)*100:.2f}%\")\n",
    "    \n",
    "    if len(anomaly_df) > 0:\n",
    "        print(\"\\nFirst 10 anomalies:\")\n",
    "        print(anomaly_df.head(10))\n",
    "    \n",
    "    # --- Visualization ---\n",
    "    create_anomaly_plots(df, anomaly_df, residuals, global_thresh)\n",
    "    \n",
    "    return anomaly_df, residuals\n",
    "\n",
    "def create_anomaly_plots(df, anomaly_df, residuals, global_thresh):\n",
    "    \"\"\"Create comprehensive anomaly detection plots\"\"\"\n",
    "    print(\"Creating visualization...\")\n",
    "    \n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Subplot 1: Full data with anomalies\n",
    "    plt.subplot(4, 1, 1)\n",
    "    # Sample data for plotting if too large\n",
    "    plot_data = df.iloc[::max(1, len(df)//5000)] if len(df) > 5000 else df\n",
    "    plt.plot(plot_data.index, plot_data['value'], label='Data', color='#36A2EB', alpha=0.6, linewidth=0.8)\n",
    "    \n",
    "    if len(anomaly_df) > 0:\n",
    "        # Sample anomalies for plotting if too many\n",
    "        plot_anomalies = anomaly_df.iloc[::max(1, len(anomaly_df)//1000)] if len(anomaly_df) > 1000 else anomaly_df\n",
    "        plt.scatter(\n",
    "            pd.to_datetime(plot_anomalies['clock'], unit='s'),\n",
    "            plot_anomalies['value'], \n",
    "            color='#FF6384', \n",
    "            label=f'Anomalies ({len(anomaly_df):,})',\n",
    "            s=15, \n",
    "            alpha=0.8,\n",
    "            zorder=5\n",
    "        )\n",
    "    \n",
    "    plt.title('Anomaly Detection Results - Full Dataset')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Residuals\n",
    "    plt.subplot(4, 1, 2)\n",
    "    plot_residuals = residuals.iloc[::max(1, len(residuals)//5000)] if len(residuals) > 5000 else residuals\n",
    "    plt.plot(plot_residuals.index, plot_residuals.values, color='orange', alpha=0.7, linewidth=0.6)\n",
    "    plt.axhline(y=global_thresh, color='red', linestyle='--', alpha=0.7, label=f'Threshold (±{global_thresh:.2f})')\n",
    "    plt.axhline(y=-global_thresh, color='red', linestyle='--', alpha=0.7)\n",
    "    plt.title('STL Residuals')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 3: Anomaly density over time\n",
    "    plt.subplot(4, 1, 3)\n",
    "    if len(anomaly_df) > 0:\n",
    "        anomaly_times = pd.to_datetime(anomaly_df['clock'], unit='s')\n",
    "        # Create hourly bins for anomaly density\n",
    "        hourly_counts = anomaly_times.dt.floor('H').value_counts().sort_index()\n",
    "        plt.plot(hourly_counts.index, hourly_counts.values, color='red', marker='o', markersize=3, linewidth=1)\n",
    "        plt.title('Anomaly Density Over Time (Hourly)')\n",
    "        plt.ylabel('Anomaly Count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No anomalies detected', ha='center', va='center', \n",
    "                transform=plt.gca().transAxes, fontsize=12)\n",
    "        plt.title('Anomaly Density Over Time')\n",
    "    \n",
    "    # Subplot 4: Statistics summary\n",
    "    plt.subplot(4, 1, 4)\n",
    "    if len(anomaly_df) > 0:\n",
    "        # Value distribution\n",
    "        plt.hist(df['value'], bins=50, alpha=0.7, color='blue', label='Normal Data', density=True)\n",
    "        plt.hist(anomaly_df['value'], bins=20, alpha=0.8, color='red', label='Anomalies', density=True)\n",
    "        plt.title('Value Distribution Comparison')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.hist(df['value'], bins=50, alpha=0.7, color='blue', label='Data Distribution')\n",
    "        plt.title('Data Value Distribution')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ANOMALY DETECTION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total data points processed: {len(df):,}\")\n",
    "    print(f\"Total anomalies detected: {len(anomaly_df):,}\")\n",
    "    print(f\"Anomaly rate: {len(anomaly_df)/len(df)*100:.3f}%\")\n",
    "    print(f\"Global residual threshold: {global_thresh:.3f}\")\n",
    "    print(f\"Data time range: {df.index.min()} to {df.index.max()}\")\n",
    "    print(f\"Processing completed successfully!\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming your DataFrame 'df' is already loaded with 'clock' and 'value' columns\n",
    "anomaly_df, residuals = detect_anomalies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c6047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbdef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "\t'https://raw.githubusercontent.com/numenta'\n",
    "\t'/NAB/master/data/realKnownCause/ambient'\n",
    "\t'_temperature_system_failure.csv')\n",
    "data = data.rename(columns={'timestamp': 'clock'})\n",
    "data\n",
    "df = data\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc99e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.robust import mad\n",
    "from joblib import Parallel, delayed, Memory\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Cache setup for intermediate results\n",
    "memory = Memory(\"cache_dir\", verbose=0)\n",
    "\n",
    "def ensure_odd_and_valid(value, minimum=3):\n",
    "    \"\"\"Ensure value is odd and >= minimum\"\"\"\n",
    "    value = max(minimum, int(value))\n",
    "    return value if value % 2 == 1 else value + 1\n",
    "\n",
    "def calculate_stl_parameters(data_length, base_period=288):  # Adjusted for 5-min intervals\n",
    "    \"\"\"Calculate valid STL parameters based on data length\"\"\"\n",
    "    if data_length < 60:\n",
    "        return None, None, None\n",
    "    period = min(base_period, max(2, data_length // 4))\n",
    "    seasonal = ensure_odd_and_valid(period // 8, 3)  # Smaller seasonal window\n",
    "    trend = ensure_odd_and_valid(period + 2, period + 2)\n",
    "    if trend >= data_length // 2:\n",
    "        period = max(2, data_length // 6)\n",
    "        trend = ensure_odd_and_valid(period + 2, period + 2)\n",
    "        seasonal = ensure_odd_and_valid(max(3, period // 4), 3)\n",
    "    return period, seasonal, trend\n",
    "\n",
    "def process_chunk(chunk_data, start_idx, end_idx, overlap_size, chunk_num, total_chunks):\n",
    "    \"\"\"Process a single chunk for STL decomposition\"\"\"\n",
    "    print(f\"Processing chunk {chunk_num + 1}/{total_chunks}...\")\n",
    "    if len(chunk_data) < 60:\n",
    "        print(f\"  Chunk {chunk_num + 1} too small, using simple detrending...\")\n",
    "        rolling_mean = chunk_data.rolling(window=min(10, len(chunk_data)//2), center=True).mean()\n",
    "        chunk_residuals = chunk_data - rolling_mean\n",
    "    else:\n",
    "        period, seasonal, trend = calculate_stl_parameters(len(chunk_data))\n",
    "        if period is None:\n",
    "            print(f\"  Chunk {chunk_num + 1} using simple detrending...\")\n",
    "            rolling_mean = chunk_data.rolling(window=min(20, len(chunk_data)//3), center=True).mean()\n",
    "            chunk_residuals = chunk_data - rolling_mean\n",
    "        else:\n",
    "            print(f\"  STL params - Period: {period}, Seasonal: {seasonal}, Trend: {trend}\")\n",
    "            robust = len(chunk_data) > 1000  # Only use robust for larger chunks\n",
    "            try:\n",
    "                stl = STL(chunk_data, period=period, seasonal=seasonal, trend=trend, robust=robust)\n",
    "                res = stl.fit()\n",
    "                chunk_residuals = res.resid\n",
    "                print(f\"  STL decomposition successful\")\n",
    "            except Exception as e:\n",
    "                print(f\"  STL failed for chunk {chunk_num + 1}: {e}\")\n",
    "                rolling_mean = chunk_data.rolling(window=min(60, len(chunk_data)//4), center=True).mean()\n",
    "                chunk_residuals = chunk_data - rolling_mean\n",
    "    return chunk_num, start_idx, end_idx, overlap_size, chunk_residuals\n",
    "\n",
    "@memory.cache\n",
    "def chunked_stl_decomposition(data, chunk_size_hours=24, overlap_hours=2):\n",
    "    \"\"\"Process STL in overlapping chunks with parallelization\"\"\"\n",
    "    chunk_size = chunk_size_hours * 60 // 5  # Adjusted for 5-min intervals\n",
    "    overlap_size = overlap_hours * 60 // 5\n",
    "    residuals = pd.Series(index=data.index, dtype=float)\n",
    "    total_chunks = (len(data) + chunk_size - 1) // chunk_size\n",
    "\n",
    "    print(f\"Processing {total_chunks} chunks...\")\n",
    "    chunks = [\n",
    "        (data.iloc[i:i + chunk_size + overlap_size], i, min(i + chunk_size + overlap_size, len(data)), overlap_size, chunk_num, total_chunks)\n",
    "        for chunk_num, i in enumerate(range(0, len(data), chunk_size))\n",
    "    ]\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(delayed(process_chunk)(*chunk) for chunk in chunks)\n",
    "\n",
    "    for chunk_num, start_idx, end_idx, overlap_size, chunk_residuals in results:\n",
    "        if chunk_num == 0:\n",
    "            residuals.iloc[start_idx:end_idx] = chunk_residuals\n",
    "        else:\n",
    "            skip_start = start_idx + overlap_size\n",
    "            residuals.iloc[skip_start:end_idx] = chunk_residuals.iloc[overlap_size:]\n",
    "\n",
    "    return residuals\n",
    "\n",
    "def compute_adaptive_stats(data, window_days=30):\n",
    "    \"\"\"Compute adaptive statistics using grouped operations\"\"\"\n",
    "    window_size = window_days * 1440 // 5  # Adjusted for 5-min intervals\n",
    "    stats_list = []\n",
    "    grouped = data.groupby('minute')['value']\n",
    "    \n",
    "    for minute, group_data in grouped:\n",
    "        if len(group_data) == 0:\n",
    "            continue\n",
    "        if len(group_data) > window_size:\n",
    "            rolling_median = group_data.rolling(window=window_size, min_periods=window_size//2).median().iloc[-1]\n",
    "            rolling_mad = group_data.rolling(window=window_size, min_periods=window_size//2).apply(lambda x: mad(x) if len(x) > 0 else 0).iloc[-1]\n",
    "        else:\n",
    "            rolling_median = group_data.median()\n",
    "            rolling_mad = mad(group_data) if len(group_data) > 1 else group_data.std()\n",
    "        stats_list.append({'minute': minute, 'median': rolling_median, 'mad_val': rolling_mad})\n",
    "    \n",
    "    return pd.DataFrame(stats_list).set_index('minute')\n",
    "\n",
    "def create_anomaly_plots(df, anomaly_df, residuals, global_thresh):\n",
    "    \"\"\"Create comprehensive anomaly detection plots\"\"\"\n",
    "    print(\"Creating visualization...\")\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    # Downsample for plotting\n",
    "    sample_rate = max(1, len(df) // 5000)\n",
    "    plot_data = df.iloc[::sample_rate]\n",
    "    plot_residuals = residuals.iloc[::sample_rate]\n",
    "\n",
    "    # Subplot 1: Full data with anomalies\n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(plot_data.index, plot_data['value'], label='Data', color='#36A2EB', alpha=0.6, linewidth=0.8)\n",
    "    if len(anomaly_df) > 0:\n",
    "        plot_anomalies = anomaly_df.iloc[::max(1, len(anomaly_df)//1000)]\n",
    "        plt.scatter(\n",
    "            pd.to_datetime(plot_anomalies['clock'], unit='s'),\n",
    "            plot_anomalies['value'], \n",
    "            color='#FF6384', \n",
    "            label=f'Anomalies ({len(anomaly_df):,})',\n",
    "            s=15, \n",
    "            alpha=0.8,\n",
    "            zorder=5\n",
    "        )\n",
    "    plt.title('Anomaly Detection Results - Full Dataset')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Subplot 2: Residuals\n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.plot(plot_residuals.index, plot_residuals.values, color='orange', alpha=0.7, linewidth=0.6)\n",
    "    plt.axhline(y=global_thresh, color='red', linestyle='--', alpha=0.7, label=f'Threshold (±{global_thresh:.2f})')\n",
    "    plt.axhline(y=-global_thresh, color='red', linestyle='--', alpha=0.7)\n",
    "    plt.title('STL Residuals')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Subplot 3: Anomaly density over time\n",
    "    plt.subplot(4, 1, 3)\n",
    "    if len(anomaly_df) > 0:\n",
    "        anomaly_times = pd.to_datetime(anomaly_df['clock'], unit='s')\n",
    "        hourly_counts = anomaly_times.dt.floor('H').value_counts().sort_index()\n",
    "        plt.plot(hourly_counts.index, hourly_counts.values, color='red', marker='o', markersize=3, linewidth=1)\n",
    "        plt.title('Anomaly Density Over Time (Hourly)')\n",
    "        plt.ylabel('Anomaly Count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No anomalies detected', ha='center', va='center', \n",
    "                transform=plt.gca().transAxes, fontsize=12)\n",
    "        plt.title('Anomaly Density Over Time')\n",
    "\n",
    "    # Subplot 4: Statistics summary\n",
    "    plt.subplot(4, 1, 4)\n",
    "    plt.hist(plot_data['value'], bins=30, alpha=0.7, color='blue', label='Normal Data', density=True)\n",
    "    if len(anomaly_df) > 0:\n",
    "        plt.hist(anomaly_df['value'], bins=10, alpha=0.8, color='red', label='Anomalies', density=True)\n",
    "    plt.title('Value Distribution Comparison')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def detect_anomalies(df):\n",
    "    \"\"\"Main anomaly detection function with improved robustness and speed\"\"\"\n",
    "    print(f\"Processing {len(df):,} data points...\")\n",
    "    \n",
    "    # Ensure data is sorted by time\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    # Add minute column to original df\n",
    "    df['minute'] = df.index.hour * 60 + df.index.minute\n",
    "    \n",
    "    # Downsample to 5-minute intervals\n",
    "    df_downsampled = df.resample('5min').mean().interpolate(method='time')\n",
    "    df_downsampled['minute'] = df_downsampled.index.hour * 60 + df_downsampled.index.minute\n",
    "    \n",
    "    # STL decomposition\n",
    "    if len(df_downsampled) > 2016:  # 1 week at 5-min intervals\n",
    "        print(\"Large dataset detected, using chunked processing...\")\n",
    "        residuals = chunked_stl_decomposition(df_downsampled['value'])\n",
    "    else:\n",
    "        print(\"Small dataset, using full STL...\")\n",
    "        period, seasonal, trend = calculate_stl_parameters(len(df_downsampled))\n",
    "        if period is not None:\n",
    "            print(f\"STL params - Period: {period}, Seasonal: {seasonal}, Trend: {trend}\")\n",
    "            robust = len(df_downsampled) > 1000\n",
    "            try:\n",
    "                stl = STL(df_downsampled['value'], period=period, seasonal=seasonal, trend=trend, robust=robust)\n",
    "                res = stl.fit()\n",
    "                residuals = res.resid\n",
    "                print(\"STL decomposition successful\")\n",
    "            except Exception as e:\n",
    "                print(f\"STL failed: {e}\")\n",
    "                rolling_mean = df_downsampled['value'].rolling(window=min(288, len(df_downsampled)//4), center=True).mean()\n",
    "                residuals = df_downsampled['value'] - rolling_mean\n",
    "        else:\n",
    "            print(\"Dataset too small for STL, using simple detrending...\")\n",
    "            rolling_mean = df_downsampled['value'].rolling(window=min(60, len(df_downsampled)//4), center=True).mean()\n",
    "            residuals = df_downsampled['value'] - rolling_mean\n",
    "    \n",
    "    # Interpolate residuals back to original index\n",
    "    residuals = residuals.reindex(df.index, method='nearest').interpolate(method='time')\n",
    "    \n",
    "    # Compute adaptive statistics\n",
    "    print(\"Computing historical patterns...\")\n",
    "    stats = compute_adaptive_stats(df_downsampled)\n",
    "    \n",
    "    # Anomaly detection\n",
    "    print(\"Detecting anomalies...\")\n",
    "    global_thresh = 5 * mad(residuals.dropna()) if len(residuals.dropna()) > 0 else 3 * df['value'].std()\n",
    "    print(f\"Global residual threshold: {global_thresh:.3f}\")\n",
    "    \n",
    "    df_merged = df.merge(stats, left_on='minute', right_index=True, how='left')\n",
    "    df_merged['median'].fillna(df['value'].median(), inplace=True)\n",
    "    df_merged['mad_val'].fillna(mad(df['value']) if len(df) > 1 else df['value'].std(), inplace=True)\n",
    "    \n",
    "    resid_condition = np.abs(residuals) > global_thresh\n",
    "    pattern_condition = np.abs(df_merged['value'] - df_merged['median']) > 2.5 * df_merged['mad_val']\n",
    "    recent_trend = df['value'].rolling(window=min(60, len(df)//10), center=True).mean()\n",
    "    trend_std = df['value'].rolling(window=min(1440, len(df)//2)).std()\n",
    "    trend_condition = np.abs(df['value'] - recent_trend) > 3 * trend_std\n",
    "    \n",
    "    condition_sum = (resid_condition.astype(int) + \n",
    "                    pattern_condition.astype(int) + \n",
    "                    trend_condition.astype(int))\n",
    "    anomaly_mask = condition_sum >= 2\n",
    "    \n",
    "    anomaly_df = pd.DataFrame({\n",
    "        'timestamp': df.index[anomaly_mask],\n",
    "        'value': df['value'][anomaly_mask]\n",
    "    })\n",
    "    anomaly_df['clock'] = anomaly_df['timestamp'].astype(np.int64) // 10**9\n",
    "    anomaly_df = anomaly_df[['clock', 'value']]\n",
    "    \n",
    "    print(f\"\\nDetected Anomalies: {len(anomaly_df):,}\")\n",
    "    print(f\"Anomaly rate: {len(anomaly_df)/len(df)*100:.2f}%\")\n",
    "    \n",
    "    if len(anomaly_df) > 0:\n",
    "        print(\"\\nFirst 10 anomalies:\")\n",
    "        print(anomaly_df.head(10))\n",
    "    \n",
    "    # Create visualization (comment out if not needed)\n",
    "    create_anomaly_plots(df, anomaly_df, residuals, global_thresh)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ANOMALY DETECTION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total data points processed: {len(df):,}\")\n",
    "    print(f\"Total anomalies detected: {len(anomaly_df):,}\")\n",
    "    print(f\"Anomaly rate: {len(anomaly_df)/len(df)*100:.3f}%\")\n",
    "    print(f\"Global residual threshold: {global_thresh:.3f}\")\n",
    "    print(f\"Data time range: {df.index.min()} to {df.index.max()}\")\n",
    "    print(f\"Processing completed successfully!\")\n",
    "    \n",
    "    return anomaly_df, residuals\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.DataFrame({'value': values, 'timestamp': timestamps})\n",
    "# df.index = pd.to_datetime(df['timestamp'])\n",
    "anomaly_df, residuals = detect_anomalies(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
